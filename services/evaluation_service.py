"""Evaluation service for responsible AI content analysis."""

from typing import Dict, List, Optional, Tuple

# Constants
UNSAFE_KEYWORDS = ["violence", "hate", "self-harm", "illegal", "explicit"]

DEFAULT_PRINCIPLES = {
    "Bias": ["gender bias", "racial bias", "stereotypes", "he is a doctor", "she is a nurse"],
    "Misinformation": ["false claims", "unverified facts", "conspiracy"],
    "Harmful Content": UNSAFE_KEYWORDS,
    "Lack of Clarity": ["vague", "unclear", "ambiguous"]
}

def generate_safe_text(prompt: str, custom_guidelines: Optional[List[str]] = None) -> Tuple[bool, str]:
    """
    Simulates a call to a safe AI model.
    Returns a tuple: (is_safe, response_text).
    """
    prompt_lower = prompt.lower()
    
    # Check for unsafe content
    if any(keyword in prompt_lower for keyword in UNSAFE_KEYWORDS):
        return (False, "I am unable to generate a response to this prompt as it may violate safety guidelines.")
    
    # Check against custom guidelines
    if custom_guidelines and any(keyword.lower() in prompt_lower for keyword in custom_guidelines):
        return (False, "I am unable to generate a response as it conflicts with the provided ethical checklist.")
            
    # Simulated safe responses for educational content
    if "photosynthesis" in prompt_lower:
        response = (
            "**Photosynthesis Explained:**\n\n"
            "Photosynthesis is the process plants use to convert light energy into chemical energy...\n\n"
            "**Practice Question:** What are the three main 'ingredients' a plant needs for photosynthesis?"
        )
        return (True, response)
    
    if "gravity" in prompt_lower:
        response = (
            "**Understanding Gravity:**\n\n"
            "Gravity is the invisible force that pulls objects toward each other...\n\n"
            "**Practice Question:** Why do you fall back to the ground when you jump?"
        )
        return (True, response)

    # Generic safe fallback response
    response = f"This is a simulated safe AI response about '{prompt}'. In a real application, this would be generated by a secure model."
    return (True, response)

def evaluate_text(text_to_evaluate: str, principles_to_check: List[str], custom_checklist: Optional[List[str]] = None) -> Dict[str, List[str]]:
    """
    Simulates evaluating text against responsible AI principles.
    Returns a dictionary with feedback.
    """
    feedback = {}
    text_lower = text_to_evaluate.lower()

    # Use default principles
    for principle in principles_to_check:
        if principle in DEFAULT_PRINCIPLES:
            found_issues = [f"Detected potential issue related to '{kw}'." for kw in DEFAULT_PRINCIPLES[principle] if kw in text_lower]
            feedback[principle] = found_issues if found_issues else ["No specific issues detected for this principle."]

    # Use custom checklist if provided
    if custom_checklist:
        custom_issues = [f"Detected text that matches custom checklist item: '{item}'." for item in custom_checklist if item.lower() in text_lower]
        feedback["Custom Educator Checklist"] = custom_issues if custom_issues else ["No issues detected based on the custom checklist."]
            
    return feedback

def load_checklist_from_file(uploaded_file) -> List[str]:
    """Reads a .txt file and returns a list of guidelines."""
    if not uploaded_file:
        return []
    try:
        checklist = [line.strip() for line in uploaded_file.getvalue().decode("utf-8").splitlines() if line.strip()]
        return checklist
    except Exception:
        return []